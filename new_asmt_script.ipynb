{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Z (NEW) ASMT DATA EXTRACT & MERGE SCRIPT**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLACE THIS SCRIPT** in the folder or directory containing the quarterly data folders.\n",
    "\n",
    "For instance, this script should be placed in the same folder or directory, and on the same level (aka, not in its own/separate folder), as the \"20181230\" and \"20190616\" data folders from ZTRAXs.\n",
    "\n",
    "----------------------------------------------------\n",
    "**This script is for ZTRANS DATA; it does the following:**\n",
    "- Works through any selected ZTRAXs data folders.\n",
    "\n",
    "*For each folder:*\n",
    "-   Takes data from selected files and only keeps selected variables from these files.\n",
    "-   Merges the individual selected ZTrans files/variables Into a single output file for each selected quarter data file.\n",
    "-   Creates a data_summary file for each merged output file which contains original and new summary statistics as well as the total number of dropped duplicate rows and execution time.\n",
    "----------------------------------------------------\n",
    "\n",
    "**TO RUN** the scipt, simply click the run button that appears near the top left of the first cell. \n",
    "\n",
    "Only run this cell, unless changes are made to any other cells. If other cells are changed, run the changed cells individually before running the main (first) cell again.\n",
    "\n",
    "**Potential improvements:**\n",
    "*This script was written for quick results/development rather than maIntainability*. All data that needs to be changed by the user to run the script on a different ZTRAX dataset should be in one place for ease of use/modification.\n",
    "Further, there is repetition in creating the dataframes from each separate file. Future development can remove these separate functions for each pre-defined file and create a single function which takes the file of Interest as its parameter. This will benefit maIntainability and allow the user to easily add additional files of Interest; right now, the script assumes we will always want to extract the same individual files from each ZTrans data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Folder:  20220429\n",
      "building\n",
      "buildingAreas\n",
      "saleData\n",
      "taxDistrict\n",
      "value\n",
      "main\n",
      "mail\n",
      "Created dataframes\n",
      "Dropped counties and duplicates\n",
      "Main: (2234759, 21)\n",
      "After mailAddress: (2234759, 32)\n",
      "After value: (2234759, 34)\n",
      "After saleData: (5241642, 38)\n",
      "After building: (5343906, 46)\n",
      "After buildingAreas: (10262664, 47)\n",
      "After taxDistrict: (26334453, 48)\n",
      "Merged\n",
      "Wrote file\n",
      "Completed 20220429. Execution time (seconds): 736.8481013774872\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import parquet #pip install parquet\n",
    "import pyarrow\n",
    "\n",
    "#[\"20181230\", \"20190319\", \"20190616\", \"20190918\", \"20191009\", \"20200102\", \"20200407\", \"20200811\", \"20201012\", \"20210111\", \"20210405\", \"20210802\", \"20210802\", \"20211018\", \"20220429\"]\n",
    "folders = [\"20220429\"]\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    for folder in folders:\n",
    "        print(\"Current Folder: \", folder)\n",
    "\n",
    "        dataframes = create_dfs_from_files(folder)\n",
    "        print(\"Created dataframes\")\n",
    "\n",
    "        dataframes[\"main\"] = drop_counties(dataframes[\"main\"])\n",
    "        print(\"Dropped counties and duplicates\")\n",
    "\n",
    "        merged = merge(dataframes)\n",
    "        print(\"Merged\")\n",
    "\n",
    "        write_file(folder, merged)\n",
    "        print(\"Wrote file\")\n",
    "\n",
    "        exec_time = time.time() - start_time\n",
    "        \n",
    "        print(\"Completed \" + folder + \". Execution time (seconds): \" + str(exec_time))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "#RUN SCRIPT\n",
    "main()\n",
    "\n",
    "#CHECK THE NV DATA FOR COUNTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs_from_files(folder):\n",
    "    dataframes = {}\n",
    "    dataframes[\"building\"] = get_building(folder)\n",
    "    dataframes[\"buildingAreas\"] = get_buildingAreas(folder)\n",
    "    dataframes[\"saleData\"] = get_saleData(folder)\n",
    "    dataframes[\"taxDistrict\"] = get_taxDistrict(folder)\n",
    "    dataframes[\"value\"] = get_value(folder)\n",
    "    dataframes[\"main\"] = get_main(folder)\n",
    "    dataframes[\"mailAddress\"] = get_mailAddress(folder)\n",
    "    return dataframes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following get methods read in the corresponding data files, selecting only the variables of Interest and giving those columns their proper names manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building(folder):\n",
    "    print(\"building\")\n",
    "    building = pd.read_csv(\n",
    "        folder + '\\\\ZAsmt\\\\Building.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 1, 4, 5, 7, 14, 19, 26, 44],\n",
    "        names=[\"RowID\", \"NoOfUnits\", \"PropertyCountyLandUseCode\", \"PropertyLandUseStndCode\", \"PropertyStateLandUseCode\", \"YearBuilt\", \"TotalBedrooms\", \"TotalActualBathCount\", \"StoryTypeStndCode\"],\n",
    "        dtype={\"RowID\": pd.StringDtype(), \"NoOfUnits\": \"Int32\", \"PropertyCountyLandUseCode\": \"category\", \"PropertyLandUseStndCode\": \"category\", \"PropertyStateLandUseCode\": \"category\", \"YearBuilt\": \"Int16\", \"TotalBedrooms\": \"category\", \"TotalActualBathCount\": \"float32\", \"StoryTypeStndCode\": \"category\"})\n",
    "\n",
    "    return building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buildingAreas(folder):\n",
    "    print(\"buildingAreas\")\n",
    "    buildingAreas = pd.read_csv(\n",
    "        folder + '\\\\ZAsmt\\\\BuildingAreas.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 4],\n",
    "        names=[\"RowID\", \"BuildingAreaSqFt\"],\n",
    "        dtype={\"RowID\": pd.StringDtype(), \"BuildingAreaSqFt\": \"Int32\"})\n",
    "\n",
    "    return buildingAreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saleData(folder):\n",
    "    print(\"saleData\")\n",
    "    saleData = pd.read_csv(\n",
    "        folder + '\\\\ZAsmt\\\\SaleData.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 2, 3, 5, 11],\n",
    "        names=[\"RowID\", \"SellerFullName\", \"BuyerFullName\", \"DocumentDate\", \"SalesPriceAmount\"],\n",
    "        dtype={\"RowID\": pd.StringDtype(), \"SellerFullName\": pd.StringDtype(), \"BuyerFullName\": pd.StringDtype(), \"DocumentDate\": pd.StringDtype(), \"SalesPriceAmount\": \"Int32\"})\n",
    "\n",
    "    return saleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxDistrict(folder):\n",
    "    print(\"taxDistrict\")\n",
    "    taxDistrict = None\n",
    "    \n",
    "    try:\n",
    "        taxDistrict = pd.read_csv(\n",
    "            folder + '\\\\ZAsmt\\\\TaxDistrict.txt',\n",
    "            sep='|',\n",
    "            on_bad_lines='skip',\n",
    "            encoding='latin-1',\n",
    "            quoting=csv.QUOTE_NONE,\n",
    "            header=None,\n",
    "            usecols=[0, 1],\n",
    "            names=[\"RowID\", \"TaxDistrictStndCode\"],\n",
    "            dtype={\"RowID\": pd.StringDtype(), \"TaxDistrictStndCode\": \"category\"})\n",
    "    except:\n",
    "        print(\"taxDistrict was empty\")\n",
    "    \n",
    "    return taxDistrict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(folder):\n",
    "    print(\"value\")\n",
    "    value = pd.read_csv(\n",
    "        folder + '\\\\ZAsmt\\\\Value.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 3, 4],\n",
    "        names=[\"RowID\", \"TotalAssessedValue\", \"AssessmentYear\"],\n",
    "        dtype={\"RowID\": pd.StringDtype(), \"TotalAssessedValue\": \"Int32\", \"AssessmentYear\": \"Int32\"})\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main(folder):\n",
    "    print(\"main\")\n",
    "    main = pd.read_csv(\n",
    "        folder + '\\\\ZAsmt\\\\Main.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 1, 2, 4, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 35, 36, 38, 39, 40, 70, 92],\n",
    "        names=[\"RowID\", \"ImportParcelID\", \"FIPS\", \"County\" , \"PropertyHouseNumber\", \"PropertyHouseNumberExt\", \"PropertyStreetPreDirectional\", \"PropertyStreetName\", \"PropertyStreetSuffix\", \"PropertyStreetPostDirectional\", \"PropertyFullStreetAddress\", \"PropertyCity\", \"PropertyZip\", \"PropertyZip4\", \"PropertyZoningSourceCode\", \"CensusTract\", \"TaxAmount\", \"TaxYear\", \"TaxDelinquencyFlag\", \"LotSizeSquareFeet\", \"BatchID\"],\n",
    "        dtype={\"RowID\": pd.StringDtype(), \"ImportParcelID\": \"Int32\", \"FIPS\": \"Int16\", \"County\": \"category\", \"PropertyHouseNumber\": pd.StringDtype(), \"PropertyHouseNumberExt\": pd.StringDtype(), \"PropertyStreetPreDirectional\": \"category\", \"PropertyStreetName\": pd.StringDtype(), \"PropertyStreetSuffix\": pd.StringDtype(), \"PropertyStreetPostDirectional\": \"category\", \"PropertyFullStreetAddress\": pd.StringDtype(), \"PropertyCity\": \"category\", \"PropertyZip\": \"category\", \"PropertyZip4\": \"category\", \"PropertyZoningSourceCode\": \"category\", \"CensusTract\": \"Int32\", \"TaxAmount\": \"float64\", \"TaxYear\": \"Int16\", \"TaxDelinquencyFlag\": \"category\", \"LotSizeSquareFeet\": \"float64\", \"BatchID\": \"Int32\"})\n",
    "\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mailAddress(folder):\n",
    "    print(\"mail\")\n",
    "    mailAddress = pd.read_csv(\n",
    "        folder + '\\\\ZAsmt\\\\MailAddress.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14],\n",
    "        names=[\"RowID\", \"MailHouseNumber\", \"MailHouseNumberExt\", \"MailStreetPreDirectional\", \"MailStreetName\", \"MailStreetSuffix\", \"MailStreetPostDirectional\", \"MailBuildingNumber\", \"MailFullStreetAddress\", \"MailCity\", \"MailZip\", \"MailZip4\"],\n",
    "        dtype={\"RowID\": pd.StringDtype(), \"MailHouseNumber\": pd.StringDtype(), \"MailHouseNumberExt\": pd.StringDtype(), \"MailStreetPreDirectional\": \"category\", \"MailStreetName\": pd.StringDtype(), \"MailStreetSuffix\": pd.StringDtype(), \"MailStreetPostDirectional\": \"category\", \"MailBuildingNumber\": pd.StringDtype(), \"MailFullStreetAddress\": pd.StringDtype(), \"MailCity\": \"category\", \"MailZip\": \"category\", \"MailZip4\": \"category\"})\n",
    "\n",
    "    return mailAddress"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFY DESIRED COUNTIES HERE. Create a new variable and assign it to a list of counties you want, all lowercase. Then, change the variable located in the isin() function to use these counties.\n",
    "\n",
    "Drops any county not in the 29 county Atlanta MSA from the main dataframe. The \"Main\" file, and the resultant dataframe, is the best source of county information from all of the files; we can assume that every property must have at least an entry in the main file.\n",
    "\n",
    "Soon, we will begin the merging process, starting with the main file. With this assumption, we can speed up the merging process by dropping now and performing a left join on the other files. The alternative would be dropping the counties after the merge, but we would still be using the \"County\" variable from the main dataframe, so it is more efficient to do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_counties(main):\n",
    "    COUNTIES_ATL = [\"barrow\", \"bartow\", \"butts\", \"carroll\", \"cherokee\", \"clayton\", \"cobb\", \"coweta\", \"dawson\", \"dekalb\", \"douglas\", \"fayette\", \"forsyth\", \"fulton\", \"gwinnett\", \"haralson\", \"heard\", \"henry\", \"jasper\", \"lamar\", \"meriwether\", \"morgan\", \"newton\", \"paulding\", \"pickens\", \"pike\", \"rockdale\", \"spalding\", \"walton\"] # 13\n",
    "    FIPS_GA = [13013, 13015, 13035, 13045, 13057, 13063, 13067, 13077, 13085, 13089, 13089, 13097, 13113, 13117, 13121, 13135, 13143, 13149, 13151, 13159, 13171, 13199, 13211, 13217, 13223, 13227, 13231, 13247, 13255, 13297]\n",
    "    COUNTIES_NC = [\"anson\", \"cabarrus\", \"gaston\", \"iredell\", \"lincoln\", \"mecklenburg\", \"rowan\", \"union\", \"chester\", \"lancaster\", \"york\"] # 37\n",
    "    COUNTIES_MD = [\"anne arundel\", \"baltimore\", \"carroll\", \"harford\", \"howard\", \"queen annes\", \"baltimore city\"] #24, baltimore (independent) -> baltimore city (looked at source)\n",
    "    COUNTIES_MN = [\"anoka\", \"carver\", \"chisago\", \"dakota\", \"hennepin\", \"isanti\", \"le sueur\", \"mille lacs\", \"ramsey\", \"scott\", \"sherburne\", \"washington\", \"wright\", \"pierce\", \"st croix\"] #27\n",
    "    COUNTIES_NV = \"clark\" #32\n",
    "    COUNTIES_WI = [\"milwaukee\", \"ozaukee\", \"washington\", \"waukesha\"] #55\n",
    "    #main = main.set_index('County')\n",
    "    #return main.loc[main.index.str.lower().isin(COUNTIES_MD)]\n",
    "    #return main.loc[main['County'].str.lower().isin(COUNTIES_MD)] # Change [COUNTIES_ATL (OR SIMILIAR)] HERE\n",
    "    #return main.loc[main['County'].str.lower() == COUNTIES_NV] # Change [COUNTIES_ATL (OR SIMILIAR)] HERE\n",
    "\n",
    "    return main.loc[main['FIPS'].isin(FIPS_GA)]\n",
    "\n",
    "    #\"lamar\", \"meriwether\", \"morgan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges the dataframes from each file, using a left join. A left join keeps all of the data from the left dataframe and adds matching data from the right dataframe, if there is any. Since we begin the merge with the main dataframe, we are utilizing the assumption that any property will at least have an entry in main. We keep this data throughout and add to it if there is any additional data with a matching RowID via left join.\n",
    "\n",
    "Finally, we drop any duplicates to reduce the data size. The number of dropped entries is also calculated and recorded in the \"data_summary\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dataframes):\n",
    "    print(\"Main:\", dataframes[\"main\"].shape)\n",
    "    merged = dataframes[\"main\"].merge(dataframes[\"mailAddress\"], how=\"left\", left_on=\"RowID\", right_on=\"RowID\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After mailAddress:\", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"value\"], how=\"left\", left_on=\"RowID\", right_on=\"RowID\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After value:\", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"saleData\"], how=\"left\", on=\"RowID\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After saleData:\", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"building\"], how=\"left\", on=\"RowID\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After building:\", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"buildingAreas\"], how=\"left\", on=\"RowID\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After buildingAreas:\", merged.shape)\n",
    "\n",
    "\n",
    "    try:\n",
    "        merged = merged.merge(dataframes[\"taxDistrict\"], how=\"left\", on=\"RowID\")\n",
    "        print(\"After taxDistrict:\", merged.shape)\n",
    "    except:\n",
    "        print(\"exluced taxDistrict\")\n",
    "\n",
    "    return merged"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes the merged dataframe to \"{year/quarter}_trans_out.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(folder, merged):\n",
    "    merged.to_csv(\"out\" + \"\\\\\" + folder + \"_asmt_out.csv\", index=False)\n",
    "    merged.to_parquet(\"out\" + \"\\\\\" + folder + \"parquet_asmt_out.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
