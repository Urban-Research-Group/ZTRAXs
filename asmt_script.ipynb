{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Z ASMT DATA EXTRACT & MERGE SCRIPT**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLACE THIS SCRIPT** in the folder or directory containing the quarterly data folders.\n",
    "\n",
    "For instance, this script should be placed in the same folder or directory, and on the same level (aka, not in its own/separate folder), as the \"20181230\" and \"20190616\" data folders from ZTRAXs.\n",
    "\n",
    "----------------------------------------------------\n",
    "**This script is for ZASMT DATA; it does the following:**\n",
    "- Works through any selected ZTRAXs data folders.\n",
    "\n",
    "*For each folder:*\n",
    "-   Takes data from selected files and only keeps selected variables from these files.\n",
    "-   Merges the individual selected ZAsmt files/variables into a single output file for each selected quarter data file.\n",
    "-   Creates a data_summary file for each merged output file which contains original and new summary statistics as well as the total number of dropped duplicate rows and execution time.\n",
    "----------------------------------------------------\n",
    "\n",
    "**TO RUN** the scipt, simply click the run button that appears near the top left of the first cell. \n",
    "\n",
    "Only run this cell, unless changes are made to any other cells. If other cells are changed, run the changed cells individually before running the main (first) cell again.\n",
    "\n",
    "**Potential improvements:**\n",
    "*This script was written for quick results/development rather than maintainability*. All data that needs to be changed by the user to run the script on a different ZTRAX dataset should be in one place for ease of use/modification.\n",
    "Further, there is repetition in creating the dataframes from each separate file. Future development can remove these separate functions for each pre-defined file and create a single function which takes the file of interest as its parameter. This will benefit maintainability and allow the user to easily add additional files of interest; right now, the script assumes we will always want to extract the same individual files from each ZAsmt data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Folder:  20181230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:3: DtypeWarning: Columns (4,9,10,11,12,13,29,30,31,32,33,34,35,38,39,40,42,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"building\"] = get_building(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:5: DtypeWarning: Columns (2,3,5,6,7,8,9,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"saleData\"] = get_saleData(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:8: DtypeWarning: Columns (10,11,13,14,15,17,18,19,20,21,25,30,31,32,34,35,37,40,43,44,45,46,47,49,52,53,54,55,56,59,62,63,65,66,67,73,74,76,77,78,80,88,89,90) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"main\"] = get_main(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:9: DtypeWarning: Columns (2,3,14,15,16,17,18,20,32,33,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"mailAddress\"] = get_mailAddress(folder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataframes\n",
      "Dropped duplicates\n",
      "Merged\n",
      "Wrote file\n",
      "Created summary\n",
      "Completed 20181230. Execution time (seconds): 491.7272672653198\n",
      "\n",
      "Current Folder:  20190319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:3: DtypeWarning: Columns (4,9,10,11,12,13,17,29,30,31,32,33,34,35,38,39,40,42,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"building\"] = get_building(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:5: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"saleData\"] = get_saleData(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:8: DtypeWarning: Columns (10,11,13,15,17,18,19,20,21,25,30,31,32,34,35,37,40,43,44,45,46,47,49,52,53,54,55,56,59,62,63,64,65,66,67,73,74,76,77,78,80,88,89,90) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"main\"] = get_main(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:9: DtypeWarning: Columns (2,3,14,15,16,17,18,20,32,33,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"mailAddress\"] = get_mailAddress(folder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataframes\n",
      "Dropped duplicates\n",
      "Merged\n",
      "Wrote file\n",
      "Created summary\n",
      "Completed 20190319. Execution time (seconds): 1024.5489990711212\n",
      "\n",
      "Current Folder:  20190616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:3: DtypeWarning: Columns (4,9,10,11,12,13,17,29,30,31,32,33,34,35,38,39,40,42,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"building\"] = get_building(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:5: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"saleData\"] = get_saleData(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:8: DtypeWarning: Columns (10,11,13,15,17,18,19,20,21,30,31,32,34,35,37,40,43,44,45,46,47,49,52,53,54,55,56,59,64,65,66,67,73,74,76,77,78,80,88,89,90) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"main\"] = get_main(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:9: DtypeWarning: Columns (2,3,13,14,15,16,17,18,20,32,33,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"mailAddress\"] = get_mailAddress(folder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataframes\n",
      "Dropped duplicates\n",
      "Merged\n",
      "Wrote file\n",
      "Created summary\n",
      "Completed 20190616. Execution time (seconds): 1615.906179189682\n",
      "\n",
      "Current Folder:  20190918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:3: DtypeWarning: Columns (4,9,10,11,12,13,17,27,29,30,31,32,33,34,35,38,39,42,44) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"building\"] = get_building(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:5: DtypeWarning: Columns (2,3,4,5,6,7,8,9,10) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"saleData\"] = get_saleData(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:8: DtypeWarning: Columns (10,11,13,14,15,18,19,20,21,30,31,32,34,35,37,40,43,44,45,46,47,49,50,52,53,54,55,56,59,64,65,66,67,73,74,76,77,78,80,88,89,90) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"main\"] = get_main(folder)\n",
      "C:\\Users\\NPOLIM~1\\AppData\\Local\\Temp\\7/ipykernel_30288/1141155359.py:9: DtypeWarning: Columns (2,3,13,14,15,16,17,18,20,32,33,34) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  dataframes[\"mailAddress\"] = get_mailAddress(folder)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataframes\n",
      "Dropped duplicates\n",
      "Merged\n",
      "Wrote file\n",
      "Created summary\n",
      "Completed 20190918. Execution time (seconds): 2350.4842069149017\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "folders = [\"20181230\", \"20190319\", \"20190616\", \"20190918\"]\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    for folder in folders:\n",
    "        print(\"Current Folder: \", folder)\n",
    "\n",
    "        dataframes = create_dfs_from_files(folder)\n",
    "        print(\"Created dataframes\")\n",
    "\n",
    "        drop_counties(dataframes[\"main\"])\n",
    "        print(\"Dropped duplicates\")\n",
    "\n",
    "        merged, num_dropped = merge(dataframes)\n",
    "        print(\"Merged\")\n",
    "\n",
    "        write_file(folder, merged)\n",
    "        print(\"Wrote file\")\n",
    "\n",
    "        exec_time = time.time() - start_time\n",
    "\n",
    "        create_summary(folder, merged, dataframes, num_dropped, exec_time)\n",
    "        print(\"Created summary\")\n",
    "        \n",
    "        print(\"Completed \" + folder + \". Execution time (seconds): \" + str(exec_time))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "#RUN SCRIPT\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs_from_files(folder):\n",
    "    dataframes = {}\n",
    "    dataframes[\"building\"] = get_building(folder)\n",
    "    dataframes[\"buildingAreas\"] = get_buildingAreas(folder)\n",
    "    dataframes[\"saleData\"] = get_saleData(folder)\n",
    "    dataframes[\"taxDistrict\"] = get_taxDistrict(folder)\n",
    "    dataframes[\"value\"] = get_value(folder)\n",
    "    dataframes[\"main\"] = get_main(folder)\n",
    "    dataframes[\"mailAddress\"] = get_mailAddress(folder)\n",
    "    return dataframes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following get methods read in the corresponding data files, selecting only the variables of interest and giving those columns their proper names manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_building(folder): \n",
    "    building = pd.read_csv(folder + '\\\\ZAsmt\\\\Building.txt', sep='|', on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "    building = building.iloc[:, [0, 1, 14, 19, 26]]\n",
    "\n",
    "    building.columns = [\"RowID\", \"NoOfUnits\", \"YearBuilt\", \"TotalBedrooms\", \"TotalActualBathCount\"]\n",
    "\n",
    "    return building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buildingAreas(folder):\n",
    "    buildingAreas = pd.read_csv(folder + '\\\\ZAsmt\\\\BuildingAreas.txt', sep='|', on_bad_lines='skip', encoding='latin-1')\n",
    "    \n",
    "    buildingAreas = buildingAreas.iloc[:, [0, 4]]\n",
    "    \n",
    "    buildingAreas.columns = [\"RowID\", \"BuildingAreaSqFt\"]\n",
    "\n",
    "    return buildingAreas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saleData(folder):\n",
    "    saleData = pd.read_csv(folder + '\\\\ZAsmt\\\\SaleData.txt', sep='|', on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "    saleData = saleData.iloc[:, [0, 2, 3, 5, 11]]\n",
    "\n",
    "    saleData.columns = [\"RowID\", \"SellerFullName\", \"BuyerFullName\", \"DocumentDate\", \"SalesPriceAmount\"]\n",
    "\n",
    "    return saleData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxDistrict(folder):\n",
    "    taxDistrict = pd.read_csv(folder + '\\\\ZAsmt\\\\TaxDistrict.txt', sep='|', on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "    taxDistrict = taxDistrict.iloc[:, [0, 1]]\n",
    "\n",
    "    taxDistrict.columns = [\"RowID\", \"TaxDistrictStndCode\"]\n",
    "\n",
    "    return taxDistrict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value(folder):\n",
    "    value = pd.read_csv(folder + '\\\\ZAsmt\\\\Value.txt', sep='|', on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "    value = value.iloc[:, [0, 3, 4]]\n",
    "\n",
    "    value.columns = [\"RowID\", \"TotalAssessedValue\", \"AssessmentYear\"]\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main(folder):\n",
    "    main = pd.read_csv(folder + '\\\\ZAsmt\\\\Main.txt', sep='|', on_bad_lines='skip', encoding='latin-1')\n",
    "\n",
    "    main = main.iloc[:, [0, 1, 4, 20, 21, 22, 23, 24, 25, 26, 27, 29, 30, 35, 36, 38, 39, 40, 70, 92]]\n",
    "\n",
    "    main.columns = [\"RowID\", \"ImportParcelID\", \"County\" , \"PropertyHouseNumber\", \"PropertyHouseNumberExt\", \"PropertyStreetPreDirectional\", \"PropertyStreetName\", \"PropertyStreetSuffix\", \"PropertyStreetPostDirectional\", \"PropertyFullStreetAddress\", \"PropertyCity\", \"PropertyZip\", \"PropertyZip4\", \"PropertyZoningSourceCode\", \"CensusTract\", \"TaxAmount\", \"TaxYear\", \"TaxDelinquencyFlag\", \"LotSizeSquareFeet\", \"BatchID\"]\n",
    "\n",
    "    return main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mailAddress(folder):\n",
    "    mailAddress = pd.read_csv(folder + '\\\\ZAsmt\\\\MailAddress.txt', sep='|', on_bad_lines='skip', encoding=\"latin-1\")\n",
    "\n",
    "    mailAddress = mailAddress.iloc[:, [0, 2, 3, 4, 5, 6, 7, 9, 10, 11, 13, 14]]\n",
    "    mailAddress.columns = [\"RowID\", \"MailHouseNumber\", \"MailHouseNumberExt\", \"MailStreetPreDirectional\", \"MailStreetName\", \"MailStreetSuffix\", \"MailStreetPostDirectional\", \"MailBuildingNumber\", \"MailFullStreetAddress\", \"MailCity\", \"MailZip\", \"MailZip4\"]\n",
    "\n",
    "    return mailAddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFY DESIRED COUNTIES HERE. Create a new variable and assign it to a list of counties you want, all lowercase. Then, change the variable located in the isin() function to use these counties.\n",
    "\n",
    "Drops any county not in the 29 county Atlanta MSA from the main dataframe. The \"Main\" file, and the resultant dataframe, is the best source of county information from all of the files; we can assume that every property must have at least an entry in the main file.\n",
    "\n",
    "Soon, we will begin the merging process, starting with the main file. With this assumption, we can speed up the merging process by dropping now and performing a left join on the other files. The alternative would be dropping the counties after the merge, but we would still be using the \"County\" variable from the main dataframe, so it is more efficient to do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_counties(main):\n",
    "    COUNTIES_ATL = [\"barrow\", \"bartow\", \"butts\", \"carroll\", \"cherokee\", \"clayton\", \"cobb\", \"coweta\", \"dawson\", \"dekalb\", \"douglas\", \"fayette\", \"forsyth\", \"fulton\", \"gwinnett\", \"haralson\", \"heard\", \"henry\", \"jasper\", \"lamar\", \"meriwether\", \"morgan\", \"newton\", \"paulding\", \"pickens\", \"pike\", \"rockdale\", \"spalding\", \"walton\"]\n",
    "    main = main.loc[main['County'].str.lower().isin(COUNTIES_ATL)] # Change (COUNTIES_ATL (OR SIMILIAR) HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges the dataframes from each file, using a left join. A left join keeps all of the data from the left dataframe and adds matching data from the right dataframe, if there is any. Since we begin the merge with the main dataframe, we are utilizing the assumption that any property will at least have an entry in main. We keep this data throughout and add to it if there is any additional data with a matching RowID via left join.\n",
    "\n",
    "Finally, we drop any duplicates to reduce the data size. The number of dropped entries is also calculated and recorded in the \"data_summary\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dataframes):\n",
    "    merged = dataframes[\"main\"].merge(dataframes[\"mailAddress\"], how=\"left\", on=\"RowID\")\n",
    "\n",
    "    merged = merged.merge(dataframes[\"saleData\"], how=\"left\", on=\"RowID\")\n",
    "    merged = merged.merge(dataframes[\"value\"], how=\"left\", on=\"RowID\")\n",
    "    merged = merged.merge(dataframes[\"building\"], how=\"left\", on=\"RowID\")\n",
    "    merged = merged.merge(dataframes[\"buildingAreas\"], how=\"left\", on=\"RowID\")\n",
    "    merged = merged.merge(dataframes[\"taxDistrict\"], how=\"left\", on=\"RowID\")\n",
    "\n",
    "    prev_size = len(merged.index)\n",
    "\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "\n",
    "    num_dropped = prev_size - len(merged.index)\n",
    "\n",
    "    return merged, num_dropped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes the merged dataframe to \"{year/quarter}_asmt_out.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(folder, merged):\n",
    "    merged.to_csv(\"out\" + \"\\\\\" + folder + \"_asmt_out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a \"data_summary.txt\" file containing the following for each quarterly data folder:\n",
    "- Execution time (seconds)\n",
    "- Number of dropped duplicates in the merged file.\n",
    "- Summary statistics for each of the original files (excluding those where this information is meaningless, ex: mailAddress): building, buildingAreas, saleData, value, and main.\n",
    "- Summary statistics for the new merged file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(folder, merged, dataframes, num_dropped, exec_time):\n",
    "    txt = open(\"out\" + \"\\\\\" + folder + \"_asmt_data_summary.txt\", 'w')\n",
    "    txt.write(\"Execution time (seconds): \" + str(exec_time))\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Number of Dropped Duplicates: \" + str(num_dropped))\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Original Data Statistics\")\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Building: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"building\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"BuildingAreas: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"buildingAreas\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"SaleData: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"saleData\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Value: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"value\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Main: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"main\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Merged Data Statistics: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(merged.describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7 (default, Sep 16 2021, 16:59:28) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
