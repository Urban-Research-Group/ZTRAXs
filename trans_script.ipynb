{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Z TRANS DATA EXTRACT & MERGE SCRIPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLACE THIS SCRIPT** in the folder or directory containing the quarterly data folders.\n",
    "\n",
    "For instance, this script should be placed in the same folder or directory, and on the same level (aka, not in its own/separate folder), as the \"20181230\" and \"20190616\" data folders from ZTRAXs.\n",
    "\n",
    "----------------------------------------------------\n",
    "**This script is for ZTRANS DATA; it does the following:**\n",
    "- Works through any selected ZTRAXs data folders.\n",
    "\n",
    "*For each folder:*\n",
    "-   Takes data from selected files and only keeps selected variables from these files.\n",
    "-   Merges the individual selected ZTrans files/variables Into a single output file for each selected quarter data file.\n",
    "-   Creates a data_summary file for each merged output file which contains original and new summary statistics as well as the total number of dropped duplicate rows and execution time.\n",
    "----------------------------------------------------\n",
    "\n",
    "**TO RUN** the scipt, simply click the run button that appears near the top left of the first cell. \n",
    "\n",
    "Only run this cell, unless changes are made to any other cells. If other cells are changed, run the changed cells individually before running the main (first) cell again.\n",
    "\n",
    "**Potential improvements:**\n",
    "*This script was written for quick results/development rather than maIntainability*. All data that needs to be changed by the user to run the script on a different ZTRAX dataset should be in one place for ease of use/modification.\n",
    "Further, there is repetition in creating the dataframes from each separate file. Future development can remove these separate functions for each pre-defined file and create a single function which takes the file of Interest as its parameter. This will benefit maIntainability and allow the user to easily add additional files of Interest; right now, the script assumes we will always want to extract the same individual files from each ZTrans data folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Folder:  20221103\n",
      "Reading ForeclosureNameAddress\n",
      "Reading propertyInfo\n",
      "Reading Main\n",
      "Reading SellerMailAddress\n",
      "Reading BuyerMailAddress\n",
      "Reading BuyerName\n",
      "Reading SellerName\n",
      "Created dataframes\n",
      "Dropped counties and duplicates\n",
      "Main df shape:  (5296545, 15)\n",
      "After merging propertyInfo:  (5296545, 28)\n",
      "After merging buyerMailAddress:  (5296545, 40)\n",
      "After merging sellerMailAddress:  (5296545, 51)\n",
      "After merging buyerName:  (5296545, 53)\n",
      "After merging sellerName:  (5296545, 55)\n",
      "After merging foreclosureName:  (5296545, 65)\n",
      "Merged\n",
      "Wrote file\n",
      "Created summary\n",
      "Completed 20221103. Execution time (seconds): 532.2796258926392\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import csv\n",
    "import parquet #pip install parquet\n",
    "import pyarrow\n",
    "\n",
    "#[\"20181230\", \"20190319\", \"20190616\", \"20190918\", \"20191009\", \"20200102\", \"20200407\", \"20200811\", \"20201012\", \"20210111\", \"20210405\", \"20210802\", \"20210802\", \"20211018\", \"20220429\"]\n",
    "folders = [\"20221103\"]\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    for folder in folders:\n",
    "        print(\"Current Folder: \", folder)\n",
    "\n",
    "        dataframes = create_dfs_from_files(folder)\n",
    "        print(\"Created dataframes\")\n",
    "\n",
    "        dataframes[\"main\"] = drop_counties(dataframes[\"main\"])\n",
    "        print(\"Dropped counties and duplicates\")\n",
    "\n",
    "        merged, num_dropped = merge(dataframes)\n",
    "        print(\"Merged\")\n",
    "\n",
    "        write_file(folder, merged)\n",
    "        print(\"Wrote file\")\n",
    "\n",
    "        exec_time = time.time() - start_time\n",
    "\n",
    "        create_summary(folder, merged, dataframes, num_dropped, exec_time)\n",
    "        print(\"Created summary\")\n",
    "        \n",
    "        print(\"Completed \" + folder + \". Execution time (seconds): \" + str(exec_time))\n",
    "        print(\"\")\n",
    "\n",
    "\n",
    "#RUN SCRIPT\n",
    "main()\n",
    "\n",
    "#CHECK THE NV DATA FOR COUNTIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dfs_from_files(folder):\n",
    "    dataframes = {}\n",
    "    dataframes[\"foreclosureName\"] = get_foreclosureName(folder)\n",
    "    dataframes[\"propertyInfo\"] = get_propertyInfo(folder)\n",
    "    dataframes[\"main\"] = get_main(folder)\n",
    "    dataframes[\"sellerMailAddress\"] = get_sellerMailAddress(folder)\n",
    "    dataframes[\"buyerMailAddress\"] = get_buyerMailAddress(folder)\n",
    "    dataframes[\"buyerName\"] = get_buyerName(folder)\n",
    "    dataframes[\"sellerName\"] = get_sellerName(folder)\n",
    "    return dataframes\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following get methods read in the corresponding data files, selecting only the variables of Interest and giving those columns their proper names manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foreclosureName(folder):\n",
    "    print(\"Reading ForeclosureNameAddress\")\n",
    "    ForeclosureNameAddress = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\ForeclosureNameAddress.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 2, 3, 4, 5, 7, 9, 11, 12, 14, 15],\n",
    "        names=[\"TransId\", \"FCMailFirstMiddleName\", \"FCMailLastName\", \"FCMailIndividualFullName\", \"FCMailNonIndividualName\", \"FCMailFullStreetAddress\", \"FCMailBuildingNumber\",\n",
    "            \"FCMailUnit\", \"FCMailCity\", \"FCMailZip\", \"FCMailZip4\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"FCMailFirstMiddleName\": pd.StringDtype(), \"FCMailLastName\": pd.StringDtype(), \"FCMailIndividualFullName\": pd.StringDtype(), \"FCMailNonIndividualName\": pd.StringDtype(), \"FCMailFullStreetAddress\": pd.StringDtype(), \"FCMailBuildingNumber\": pd.StringDtype(),\n",
    "            \"FCMailUnit\": pd.StringDtype(), \"FCMailCity\": \"category\", \"FCMailZip\": \"category\", \"FCMailZip4\": \"category\"})\n",
    "\n",
    "    return ForeclosureNameAddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_propertyInfo(folder):\n",
    "    print(\"Reading PropertyInfo\")\n",
    "    PropertyInfo = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\PropertyInfo.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 8, 9, 10, 11, 12, 13, 14, 15, 16, 18, 19, 64],\n",
    "        names=[\"TransId\", \"PropertyHouseNumber\", \"PropertyHouseNumberExt\", \"PropertyStreetPreDirectional\", \"PropertyStreetName\", \"PropertyStreetSuffix\", \"PropertyStreetPostDirectional\",\n",
    "            \"PropertyBuildingNumber\", \"PropertyFullStreetAddress\", \"PropertyCity\", \"PropertyZip\", \"PropertyZip4\", \"ImportParcelID\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"PropertyHouseNumber\": pd.StringDtype(), \"PropertyHouseNumberExt\": pd.StringDtype(), \"PropertyStreetPreDirectional\": \"category\", \"PropertyStreetName\": pd.StringDtype(), \"PropertyStreetSuffix\": pd.StringDtype(), \"PropertyStreetPostDirectional\": \"category\",\n",
    "            \"PropertyBuildingNumber\": pd.StringDtype(), \"PropertyFullStreetAddress\": pd.StringDtype(), \"PropertyCity\": \"category\", \"PropertyZip\": \"category\", \"PropertyZip4\": \"category\", \"ImportParcelID\": \"Int32\"})\n",
    "\n",
    "    return PropertyInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main(folder):\n",
    "    print(\"Reading Main\")\n",
    "    Main = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\Main.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 1, 3, 4, 16, 17, 18, 24, 25, 30, 32, 62, 104, 105, 127],\n",
    "        names=[\"TransId\", \"FIPS\", \"County\", \"DocumentTypeStndCode\", \"DataClassStndCode\", \"DocumentDate\", \"SignatureDate\", \"SalesPriceAmount\", \"SalesPriceAmountStndCode\", \"IntraFamilyTransferFlag\",\n",
    "            \"PropertyUseStndCode\", \"LoanTypeStndCode\", \"TotalDelinquentAmount\", \"DelinquentAsOfDate\", \"BatchID\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"FIPS\": \"Int32\", \"County\": \"category\", \"DocumentTypeStndCode\": \"category\", \"DataClassStndCode\": \"category\", \"DocumentDate\": pd.StringDtype(), \"SignatureDate\": pd.StringDtype(), \"SalesPriceAmount\": \"Float32\", \"SalesPriceAmountStndCode\": \"category\", \"IntraFamilyTransferFlag\": \"category\",\n",
    "            \"PropertyUseStndCode\": \"category\", \"LoanTypeStndCode\": \"category\", \"TotalDelinquentAmount\": \"Float32\", \"DelinquentAsOfDate\": pd.StringDtype(), \"BatchID\": \"Int32\"})\n",
    "\n",
    "    return Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sellerMailAddress(folder):\n",
    "    print(\"Reading SellerMailAddress\")\n",
    "    SellerMailAddress = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\SellerMailAddress.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15],\n",
    "        names=[\"TransId\", \"SellerMailHouseNumber\", \"SellerMailHouseNumberExt\", \"SellerMailStreetPreDirectional\", \"SellerMailStreetName\", \"SellerMailStreetSuffix\", \"SellerMailStreetPostDirectional\",\n",
    "            \"SellerMailBuildingNumber\", \"SellerMailFullStreetAddress\", \"SellerMailCity\", \"SellerMailZip\", \"SellerMailZip4\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"SellerMailHouseNumber\": pd.StringDtype(), \"SellerMailHouseNumberExt\": pd.StringDtype(), \"SellerMailStreetPreDirectional\": \"category\", \"SellerMailStreetName\": pd.StringDtype(), \"SellerMailStreetSuffix\": pd.StringDtype(), \"SellerMailStreetPostDirectional\": \"category\",\n",
    "            \"SellerMailBuildingNumber\": pd.StringDtype(), \"SellerMailFullStreetAddress\": pd.StringDtype(), \"SellerMailCity\": \"category\", \"SellerMailZip\": \"category\", \"SellerMailZip4\": \"category\"})\n",
    "\n",
    "    return SellerMailAddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_buyerMailAddress(folder):\n",
    "    print(\"Reading BuyerMailAddress\")\n",
    "    BuyerMailAddress = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\BuyerMailAddress.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 3, 4, 5, 6, 7, 8, 10, 11, 12, 14, 15],\n",
    "        names=[\"TransId\", \"BuyerMailHouseNumber\", \"BuyerMailHouseNumberExt\", \"BuyerMailStreetPreDirectional\", \"BuyerMailStreetName\", \"BuyerMailStreetSuffix\", \"BuyerMailStreetPostDirectional\",\n",
    "            \"BuyerMailBuildingNumber\", \"BuyerMailFullStreetAddress\", \"BuyerMailCity\", \"BuyerMailZip\", \"BuyerMailZip4\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"BuyerMailHouseNumber\": pd.StringDtype(), \"BuyerMailHouseNumberExt\": pd.StringDtype(), \"BuyerMailStreetPreDirectional\": \"category\", \"BuyerMailStreetName\": pd.StringDtype(), \"BuyerMailStreetSuffix\": pd.StringDtype(), \"BuyerMailStreetPostDirectional\": \"category\",\n",
    "            \"BuyerMailBuildingNumber\": pd.StringDtype(), \"BuyerMailFullStreetAddress\": pd.StringDtype(), \"BuyerMailCity\": \"category\", \"BuyerMailZip\": \"category\", \"BuyerMailZip4\": \"category\"})\n",
    "\n",
    "    return BuyerMailAddress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buyerName(folder):\n",
    "    print(\"Reading BuyerName\")\n",
    "    BuyerName = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\BuyerNameAgg.csv',\n",
    "        dtype={\"TransId\": \"Int32\", \"BuyerIndividualFullName\": pd.StringDtype(), \"BuyerNonIndividualName\": pd.StringDtype()})\n",
    "\n",
    "    return BuyerName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sellerName(folder):\n",
    "    print(\"Reading SellerName\")\n",
    "    SellerName = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\SellerNameAgg.csv',\n",
    "        dtype={\"TransId\": \"Int32\", \"SellerIndividualFullName\": pd.StringDtype(), \"SellerNonIndividualName\": pd.StringDtype()})\n",
    "\n",
    "    return SellerName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_propertyInfo(folder):\n",
    "    print(\"Reading propertyInfo\")\n",
    "    SellerName = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\PropertyAgg.csv',\n",
    "        dtype={\"TransId\": \"Int32\", \"PropertyHouseNumber\": pd.StringDtype(), \"PropertyHouseNumberExt\": pd.StringDtype(), \"PropertyStreetPreDirectional\": \"category\", \"PropertyStreetName\": pd.StringDtype(), \"PropertyStreetSuffix\": pd.StringDtype(), \"PropertyStreetPostDirectional\": \"category\",\n",
    "            \"PropertyBuildingNumber\": pd.StringDtype(), \"PropertyFullStreetAddress\": pd.StringDtype(), \"PropertyCity\": \"category\", \"PropertyZip\": \"category\", \"PropertyZip4\": \"category\", \"ImportParcelID\": \"Int32\"})\n",
    "\n",
    "    return SellerName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_buyerMailAddress(folder):\n",
    "    print(\"Reading BuyerMailAddress\")\n",
    "    BuyerMailAddress = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\BuyerMailAgg.csv',\n",
    "        dtype={\"TransId\": \"Int32\", \"BuyerMailHouseNumber\": pd.StringDtype(), \"BuyerMailHouseNumberExt\": pd.StringDtype(), \"BuyerMailStreetPreDirectional\": \"category\", \"BuyerMailStreetName\": pd.StringDtype(), \"BuyerMailStreetSuffix\": pd.StringDtype(), \"BuyerMailStreetPostDirectional\": \"category\",\n",
    "            \"BuyerMailBuildingNumber\": pd.StringDtype(), \"BuyerMailFullStreetAddress\": pd.StringDtype(), \"BuyerMailCity\": \"category\", \"BuyerMailZip\": \"category\", \"BuyerMailZip4\": \"category\"})\n",
    "\n",
    "    return BuyerMailAddress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_buyerName(folder):\n",
    "    print(\"Reading BuyerName\")\n",
    "    BuyerName = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\BuyerName.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 1, 2, 3, 4, 5],\n",
    "        names=[\"TransId\", \"BuyerFirstMiddleName\", \"BuyerLastName\", \"BuyerIndividualFullName\", \"BuyerNonIndividualName\", \"BuyerNameSequenceNumber\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"BuyerFirstMiddleName\": pd.StringDtype(), \"BuyerLastName\": pd.StringDtype(), \"BuyerIndividualFullName\": pd.StringDtype(), \"BuyerNonIndividualName\": pd.StringDtype(), \"BuyerNameSequenceNumber\": \"Int16\"})\n",
    "\n",
    "    return BuyerName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_sellerName(folder):\n",
    "    print(\"Reading SellerName\")\n",
    "    SellerName = pd.read_csv(\n",
    "        folder + '\\\\ZTrans\\\\SellerName.txt',\n",
    "        sep='|',\n",
    "        on_bad_lines='skip',\n",
    "        encoding='latin-1',\n",
    "        quoting=csv.QUOTE_NONE,\n",
    "        header=None,\n",
    "        usecols=[0, 1, 2, 3, 4],\n",
    "        names=[\"TransId\", \"SellerFirstMiddleName\", \"SellerLastName\", \"SellerIndividualFullName\", \"SellerNonIndividualName\"],\n",
    "        dtype={\"TransId\": \"Int32\", \"SellerFirstMiddleName\": pd.StringDtype(), \"SellerLastName\": pd.StringDtype(), \"SellerIndividualFullName\": pd.StringDtype(), \"SellerNonIndividualName\": pd.StringDtype()})\n",
    "\n",
    "    return SellerName"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODIFY DESIRED COUNTIES HERE. Create a new variable and assign it to a list of counties you want, all lowercase. Then, change the variable located in the isin() function to use these counties.\n",
    "\n",
    "Drops any county not in the 29 county Atlanta MSA from the main dataframe. The \"Main\" file, and the resultant dataframe, is the best source of county information from all of the files; we can assume that every property must have at least an entry in the main file.\n",
    "\n",
    "Soon, we will begin the merging process, starting with the main file. With this assumption, we can speed up the merging process by dropping now and performing a left join on the other files. The alternative would be dropping the counties after the merge, but we would still be using the \"County\" variable from the main dataframe, so it is more efficient to do it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_counties(main):\n",
    "    COUNTIES_ATL = [\"barrow\", \"bartow\", \"butts\", \"carroll\", \"cherokee\", \"clayton\", \"cobb\", \"coweta\", \"dawson\", \"dekalb\", \"douglas\", \"fayette\", \"forsyth\", \"fulton\", \"gwinnett\", \"haralson\", \"heard\", \"henry\", \"jasper\", \"lamar\", \"meriwether\", \"morgan\", \"newton\", \"paulding\", \"pickens\", \"pike\", \"rockdale\", \"spalding\", \"walton\"] # 13\n",
    "    FIPS_GA = [13013, 13015, 13035, 13045, 13057, 13063, 13067, 13077, 13085, 13089, 13089, 13097, 13113, 13117, 13121, 13135, 13143, 13149, 13151, 13159, 13171, 13199, 13211, 13217, 13223, 13227, 13231, 13247, 13255, 13297]\n",
    "    COUNTIES_NC = [\"anson\", \"cabarrus\", \"gaston\", \"iredell\", \"lincoln\", \"mecklenburg\", \"rowan\", \"union\", \"chester\", \"lancaster\", \"york\"] # 37\n",
    "    COUNTIES_MD = [\"anne arundel\", \"baltimore\", \"carroll\", \"harford\", \"howard\", \"queen annes\", \"baltimore city\"] #24, baltimore (independent) -> baltimore city (looked at source)\n",
    "    FIPS_MD = [24003, 24005, 24013, 24025, 24027, 24035, 24510]\n",
    "    COUNTIES_MN = [\"anoka\", \"carver\", \"chisago\", \"dakota\", \"hennepin\", \"isanti\", \"le sueur\", \"mille lacs\", \"ramsey\", \"scott\", \"sherburne\", \"washington\", \"wright\", \"pierce\", \"st croix\"] #27\n",
    "    FIPS_MN = [27003, 27019, 27025, 27037, 27053, 27059, 27079, 27095, 27123, 27139, 27141, 27163, 27171]\n",
    "    COUNTIES_NV = \"clark\" #32\n",
    "    FIPS_NV = [32003]\n",
    "    COUNTIES_WI = [\"milwaukee\", \"ozaukee\", \"washington\", \"waukesha\"] #55\n",
    "    #main = main.set_index('County')\n",
    "    #return main.loc[main.index.str.lower().isin(COUNTIES_MD)]\n",
    "    #return main.loc[main['County'].str.lower().isin(COUNTIES_MD)] # Change [COUNTIES_ATL (OR SIMILIAR)] HERE\n",
    "    #return main.loc[main['County'].str.lower() == COUNTIES_NV] # Change [COUNTIES_ATL (OR SIMILIAR)] HERE\n",
    "\n",
    "    return main.loc[main['FIPS'].isin(FIPS_GA)]\n",
    "\n",
    "    #\"lamar\", \"meriwether\", \"morgan\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merges the dataframes from each file, using a left join. A left join keeps all of the data from the left dataframe and adds matching data from the right dataframe, if there is any. Since we begin the merge with the main dataframe, we are utilizing the assumption that any property will at least have an entry in main. We keep this data throughout and add to it if there is any additional data with a matching RowID via left join.\n",
    "\n",
    "Finally, we drop any duplicates to reduce the data size. The number of dropped entries is also calculated and recorded in the \"data_summary\" file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(dataframes):\n",
    "    print(\"Main df shape: \", dataframes[\"main\"].shape)\n",
    "    merged = dataframes[\"main\"].merge(dataframes[\"propertyInfo\"], how=\"left\", left_on=\"TransId\", right_on=\"TransId\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    prev_size = len(merged.index)\n",
    "    print(\"After merging propertyInfo: \", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"buyerMailAddress\"], how=\"left\", left_on=\"TransId\", right_on=\"TransId\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After merging buyerMailAddress: \", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"sellerMailAddress\"], how=\"left\", on=\"TransId\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After merging sellerMailAddress: \", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"buyerName\"], how=\"left\", on=\"TransId\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After merging buyerName: \", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"sellerName\"], how=\"left\", on=\"TransId\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After merging sellerName: \", merged.shape)\n",
    "\n",
    "    merged = merged.merge(dataframes[\"foreclosureName\"], how=\"left\", on=\"TransId\")\n",
    "    merged.drop_duplicates(inplace=True)\n",
    "    print(\"After merging foreclosureName: \", merged.shape)\n",
    "\n",
    "    num_dropped = prev_size - len(merged.index)\n",
    "\n",
    "    return merged, num_dropped"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes the merged dataframe to \"{year/quarter}_trans_out.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(folder, merged):\n",
    "    merged.to_csv(\"out\" + \"\\\\\" + folder + \"_trans_out.csv\", index=False)\n",
    "    merged.to_parquet(\"out\" + \"\\\\\" + folder + \"_parquet_trans_out.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a \"data_summary.txt\" file containing the following for each quarterly data folder:\n",
    "- Execution time (seconds)\n",
    "- Number of dropped duplicates in the merged file.\n",
    "- Summary statistics for each of the original files (excluding those where this information is meaningless, ex: mailAddress): building, buildingAreas, saleData, value, and main.\n",
    "- Summary statistics for the new merged file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary(folder, merged, dataframes, num_dropped, exec_time):\n",
    "    txt = open(\"out\" + \"\\\\\" + folder + \"_trans_data_summary.txt\", 'w')\n",
    "    txt.write(\"Execution time (seconds): \" + str(exec_time))\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Change in observations from original file: \" + str(num_dropped))\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Original Data Statistics\")\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Main: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"main\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"BuyerMailAddress: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"buyerMailAddress\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"SellerMailAddress: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"sellerMailAddress\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"PropertyInfo: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"propertyInfo\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"BuyerName: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"buyerName\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "    \n",
    "    txt.write(\"SellerName: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"sellerName\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "    \n",
    "    txt.write(\"ForeclosureNameAddress: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(dataframes[\"foreclosureName\"].describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.write(\"Merged Data Statistics: \")\n",
    "    txt.write(\"\\n\")\n",
    "    txt.write(merged.describe().round(2).to_string())\n",
    "    txt.write(\"\\n\\n\")\n",
    "\n",
    "    txt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
